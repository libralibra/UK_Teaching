<!doctype html>
<html lang="en">

    <head>
        <title></title>

        <!-- charset setting -->
        <meta charset="utf-8">

        <!-- meta data -->
        <meta name="author" content="Dr Xu Zhang">
        <meta name="generator" content="VS Code + Reveal.js + Markdown + Mermaid">

        <!-- disable search engine robots -->
        <meta name="robots" content="none">

        <!-- viewport settings -->
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta name="apple-mobile-web-app-capable" content="yes">
        <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

        <!-- all css files -->
        <link rel="stylesheet" href="../../../assets/dist/css/master.css">

        <!-- define data but update later -->
        <script>
            const authorData = new Map([
                ['course', 'Classical Artificial Intelligence'],
                ['coursecode', 'COMP712'],
                ['week', '5'],
                ['topic', ' - Monte Carlo Tree Search'],
            ]);
        </script>
        <script id="header_pdf_func" src="../../../assets/dist/js/header.js"></script>
    </head>

    <body>
        <!-- pdf printing by showing a corner strip+
                    put pdf button before anything else, as the z-index makes sure later elements has a 
                    higher z-index than those before. In overview mode, the slides should be in the front.
                -->
        <div class="pdf_div">
            <div class="pdf_link" title="Save Slides as PDF" onclick="GeneratePDF();"><a class="link_no_change" href='#'>⇩PDF</a></div>
        </div>

        <!-- main content of the slides -->
        <div class="reveal">
            <!-- Any section element inside of this container is displayed as a slide -->
            <div class="slides">
                <section id="chap_title">
                    <section id="title">
                        <h1><span class="course"></span></h1>
                        <h3><span class="coursecode orange"></span></h3>
                        <p><span class="week bigtext"></span><span class="topic bigtext"></span></p>
                        <img src="../common/digitap2.png" alt="digital attendance reminder" class="r-stretch" />
                        <p>
                            <span class="fullname cyan"></span> <br>
                            <span class="uni"></span> v. <span class="year"></span>
                        </p>
                    </section>

                    <section id="overview">
                        <h1>Overview</h1>
                        <ul>
                            <li>Monte Carlo Method: what is it, how does it work</li>
                            <li>Search Strategy: expected return, upper confidence bound</li>
                            <li>Monte Carlo Tree Search: precess, UCB for Trees (UCT)</li>
                            <li>Challenges in Big Games</li>
                        </ul>
                    </section>
                </section>

                <section id="chap_monte_carlo">
                    <section id="monte_carlo_title">
                        <h1 class="r-frame-text">Monte Carlo Methods</h1>
                    </section>

                    <section id="expected_value">
                        <h1>Expected Value</h1>
                        <ul>
                            <li>Let $X$ be a <span class="blue">random variable</span></li>
                            <li>Let $p(x)$ be the <span class="lime">probability</span> that $X$ has value $x$</li>
                            <li>Then, the <span class="pink">expected value</span> of $X$ is
                                <dd>\[
                                    \sum_x{xp(x)}
                                    \]
                                </dd>
                            </li>
                        </ul>
                    </section>

                    <section id="expected_value_example">
                        <h1>Expected Value - Example</h1>
                        <ul>
                            <li class="fragment">Suppose that a slot machine pays out
                                <dd><span class="lime">$£1$</span> with probability <span class="skyblue">$0.05$</span>,<br>
                                    <span class="lime">$£5$</span> with probability <span class="skyblue">$0.03$</span>,<br>
                                    <span class="lime">$£10$</span> with probability <span class="skyblue">$0.02$</span>,<br>
                                    <span class="lime">nothing</span> with probability <span class="skyblue">$0.9$</span>.
                                </dd>
                            </li>
                            <li class="fragment">The expected payout is
                                <dd><span class="lime">$1$</span>$\times$<span class="skyblue">$0.05$</span>$+$<span class="lime">$5$</span>$\times$<span class="skyblue">$0.03$</span>$+$<span class="lime">$10$</span>$\times$<span class="skyblue">$0.02$</span>$+$<span class="lime">$0$</span>$\times$<span class="skyblue">$0.90$</span>$=$<span class="pink">$0.40$</span></dd>
                            </li>
                            <li class="fragment">On average, if you play the machine <span class="blue">$N$</span> times, you will win <span class="pink">$£0.40*N$</span></li>
                        </ul>
                    </section>

                    <section id="randomness">
                        <h1>"Randomness" in Computing</h1>
                        <ul>
                            <li>Digital computers are <span class="blue">deterministic</span>, so there's no such thing as true randomness
                                <dd>- Cryptographically secure systems use an external source of randomness, e.g. atmospheric noise, radioactive decay, etc.</dd>
                            </li>
                            <li>What we actually have are <span class="lime">pseudo-random number generators (PRNGs)</span></li>
                            <li>A PRNG is an algorithm which gives an <span class="pink">unpredictable</span> sequence of numbers based on a <span class="blue">seed</span></li>
                            <li>Sequence is <span class="lime">uniformly distributed</span>, i.e. all numbers have equal probability</li>
                            <li>Seed is generally based on some source of <span class="orange">entropy</span>, e.g. system clock, mouse input, electronic noise</li>
                        </ul>
                    </section>

                    <section id="monte_carlo">
                        <h1>Monte Carlo Methods</h1>
                        <ul>
                            <li>In computing, a <span class="yellow">Monte Carlo Method</span> is an algorithm based on <span class="orange">averaging over random samples</span></li>
                            <li>The <span class="orange">average</span> over a large number of samples is a good approximation of the <span class="pink">expected value</span></li>
                            <li>Used for <span class="grey">quickly approximating</span> quantities over <span class="skyblue">large domains</span></li>
                            <li>Generally designed to <span class="blue">converge in the limit</span>
                                <ul>
                                    <li>an <span class="silver">infinite</span> number of samples would give an <span class="purple">exact</span> answer</li>
                                    <li>As the <span class="golden">number of samples</span> increases, the <span class="cyan">accuracy</span> of the answer improves</li>
                                </ul>
                            </li>
                            <li>Applications in physics, engineering, finance, weather forecasting, graphics, ...</li>
                        </ul>
                    </section>

                    <section id="monte_carlo_pi">
                        <h1>Monte Carlo Method Example</h1>
                        <img class="r-stretch" src="img/monte_carlo_pi.gif" alt="calculating Pi using Monte Carlo simulation">
                        <p>$\frac{inside\_point\_number}{total\_number}=\frac{\pi}{4}$</p>
                    </section>
                </section>

                <section id="chap_ucb">
                    <section id="ucb_title">
                        <h1 class="r-frame-text">Search Strategy</h1>
                    </section>

                    <section id="multi_arm">
                        <h1>The Multi-armed Bandit Problem</h1>
                        <div class="row no-margin-bottom no-margin-top">
                            <div class="col-50 text-left">
                                <ul>
                                    <li>Pull one arm at a time</li>
                                    <li>Each arm has a different chance to win</li>
                                    <li>Random/noisy reward signal</li>
                                    <li>Pulling any arm either wins or loses</li>
                                    <li>Maximise the total reward after each move (expected return)</li>
                                </ul>
                                </p>
                            </div>
                            <div class="col-50">
                                <img src="img/multi_arm.png" alt="The multi-armed bandit problem"><br>
                                <a class="tinytext" href="https://tinyurl.com/58n984kp" target="_blank">https://tinyurl.com/58n984kp</a>
                            </div>
                        </div>
                    </section>

                    <section id="expected_return">
                        <h1>Expected Return</h1>
                        <ul>
                            <li><span class="orange">Action value</span> $\implies$ to decide which arm is the best action ($a$) we define the value of taking the action as: $q_t(a)=E(R_t \vert A_t = a)$
                                <dd>- Maximise the expected reward ($R_t$) by selecting the action ($a$) with the highest action-value ($q_t(a)$)</dd>
                            </li>
                            <li><span class="pink">Action value estimate</span> $\implies$ since the action value is unknown to the agent it can be estimated by: $Q_t(a)=\frac{\sum_{T=0}^{T=t} R_T}{N_t(a)}$
                                <dd>- The sample-average: the total reward ($\sum_{T=0}^{T=t} R_T$) is divided by the number of actions ($N_t(a)$)</dd>
                            </li>
                        </ul>
                    </section>

                    <!-- the next 3 slides refers to https://www.geeksforgeeks.org/upper-confidence-bound-algorithm-in-reinforcement-learning/ -->
                    <section id="explore_exploit">
                        <h1>Exploration <span class="italic">vs</span> Exploitation Dilemma</h1>
                        <ul>
                            <li><span class="blue">Greedy Action</span>: By choose the largest estimated value, the agent exploits its current knowledge.</li>
                            <li><span class="orange">Non-Greedy Action</span>: Do not choose the largest estimated value and hope to gain more information about the other actions.</li>
                            <li><span class="lime">Exploration</span>: It allows the agent to improve its knowledge about each action. Hopefully, leading to a <span class="purple">long-term benefit</span>.</li>
                            <li><span class="yellow">Exploitation</span>: Try to get the most reward for <span class="cyan">short-term benefit</span>. A pure greedy action selection can lead to <span class="red">sub-optimal</span> behaviour.</li>
                        </ul>
                        <p class="fragment skyblue">A dilemma occurs between exploration and exploitation because an agent can not choose to both explore and exploit at the same time.</p>
                    </section>

                    <section id="ucb">
                        <h1>Upper Confidence Bound (UCB)</h1>
                        <ul>
                            <li>Balance <span class="lime">exploitation</span> with <span class="orange">exploration</span></li>
                            <li>Inherent uncertainty in the accuracy of the action-value estimates</li>
                            <li>Select the arm that maximises:
                                <dd class="text-center">
                                    $A_t=argmax_a\left(Q_t(a)+c\sqrt{\frac{ln(t)}{N_t(a)}}\right)$
                                </dd>
                                <dd>where $c$ is a trade-off between exploration and exploitation. If $c=0$ then you only consider $Q_t(a)$ (no exploration); if $c→\infty$ then you only consider exploration term (<span class="cyan">UCB</span>).</dd>
                            </li>
                            <li>Select the action that has:<dd> the highest estimated action-value <span class="purple">plus</span> the UCB exploration term.</dd>
                            </li>
                        </ul>
                    </section>

                    <section id="ucb_ci">
                        <h1>UCB with Confidence Interval</h1>
                        <p class="text-left">$Q(A)$ represents the current action-value estimate for action $A$. <br>The brackets represent a confidence interval which says that we are confident that the actual $Q(A)$ lies somewhere in this region.</p>
                        <img class="r-stretch" src="img/ucb_1.png" alt="UCB">
                    </section>

                    <section id="ucb_example_1" data-auto-animate>
                        <h1>UCB Example</h1>
                        optimistically pick action $A$ to either get the highest reward or learn about an action we know least about.
                        <img class="r-stretch" src="img/ucb_2.png" alt="UCB example">
                    </section>

                    <section id="ucb_example_2" data-auto-animate>
                        <h1>UCB Example (cont.)</h1>
                        Select the action $B$ since $Q(B)$ has the highest UCB, even though the confidence interval is small.
                        <img class="r-stretch" src="img/ucb_3.png" alt="UCB example">
                    </section>
                </section>

                <section id="chap_mcts">
                    <section id="MCTS_title">
                        <h1 class="r-frame-text">Monte Carlo Tree Search</h1>
                        <img src="img/slots.gif" alt="Slot Game">
                    </section>

                    <section id="game_decision_revisted">
                        <h1>Game Decision Revisited</h1>
                        <div class="mermaid">
                            <pre>
                                %%{init: {'theme': 'dark', 'themeVariables': { 'darkMode': true }}}%%
                                graph TD
                                A(Current Position) -- Move A --> B(PA)
                                A(Current Position) -- Move B --> C(PB)
                                A(Current Position) -- Move C --> D(PD)
                                C -. playout policy .-> E(Terminal: win=1, lose=0)
                                E -- Reward for Move B --> C
                            </pre>
                        </div>
                        <p class="text-left">PB to Terminal (dash line) represents the (possibly long) <span class="blue">playout policy</span>. Reward feedbacks to PB.<br>Minimax algorithm requires the expansion of whole game tree. So we may:<br>
                            - expand the game tree only <span class="pink">up to certain</span> threshold <span class="pink">depth $d$</span><br>
                            - prune the game tree using <a href="https://www.wikiwand.com/en/Alpha%E2%80%93beta_pruning" alt="Alpha beta pruning algorithm" target="_blank">Alpha-Beta Pruning Algorithm</a></p>
                    </section>

                    <section id="mcts_process">
                        <h1>Monte Carlo Tree Search (MCTS) Process</h1>
                        <div class="mermaid">
                            <pre>
                                %%{init: {'theme': 'dark', 'themeVariables': { 'darkMode': true }}}%%
                                graph TD
                                A(Current Position) -- Move A --> B(PA)
                                A(Current Position) -- Move B --> C(PB)
                                A(Current Position) -- Move C --> D(PC)
                                C -. tree policy .-> E(PD)
                                C -- Move D --> E(PD)
                                E -. default policy .-> F(Terminal: win=1, lose=0)
                                F -- Reward --> C
                                F -- Reward --> E
                            </pre>
                        </div>
                        <ul class="float-left">Now, the <span class="blue">playout policy</span> to select the "possible" best move include 2 steps:
                            <li>Select node PD (Move D) via <span class="lime">tree policy</span>: UCB</li>
                            <li>From PD to Terminal using the <span class="skyblue">default policy</span>: uniform random</li>
                            <li>Reward feedbacks to both PB and PD</li>
                        </ul>
                    </section>

                    <!-- refer to https://int8.io/monte-carlo-tree-search-beginners-guide/ -->
                    <section id="monte_carlo_tree_search">
                        <h1>UCB for MCTS - UCT</h1>
                        <div class="row">
                            <div class="col-50">
                                <ul>
                                    <li>Iteratively build a partial search tree, one node per iteration</li>
                                    <li>Monte Carlo (randomness) evaluation for non-terminal nodes</li>
                                    <li><span class="lime">Asymmetric</span>: balance between exploitation and exploration with unbalanced tree topology</li>
                                    <li><span class="orange">Anytime</span>: can do as many (or as few) iteration as we want</li>
                                    <li><span class="purple">Aheuristic</span>: no prior knowledge required except rules</li>
                                </ul>
                            </div>
                            <div class="fragment col-50">
                                <img src="img/mcts.png" alt="MCTS" class="r-stretch"><br>
                                <span class="smalltext text-left">
                                    <span class="lime">$UCT(v_i,v)=\frac{W_{v_i}}{N_{v_i}}+c\sqrt{\frac{log(N_v)}{N_{v_i}}}$</span>
                                    <br>
                                    $c$: exploration parameter (theoretically $=\sqrt{2}$)
                                    $v_i$: $i$-th child node of node $v$<br>
                                    $W_{v_i}$: the number of wins from node $v_i$<br>
                                    $N_{v_i}$: the number of visits to node $v_i$<br>
                                    $N_v$: the number of visits to it's parent node $v$<br>
                                    <span class="golden smalltext">How does $N_{v_i}$ affect the UCT value?</span>
                                </span>
                            </div>
                        </div>
                    </section>

                    <section id="why_mcts">
                        <h1>Why MCTS?</h1>
                        <ul>
                            <li>MCTS simulates the process how we play games: we do not list all moves but the "most possible" moves of the opponent after we make our "most possible" moves and so on... </li>
                            <li>Effectively search a large "space" without prior knowledge</li>
                            <li>Estimate the value of each possible move by random simulation</li>
                            <li>Move towards the best strategy</li>
                        </ul>
                    </section>

                    <section id="mcts_process">
                        <h1>MCTS Process</h1>
                        <p class="text-left">Each iteration of MCTS includes 4 steps:</p>
                        <img src="img/mctc_steps.png" alt="MCTS steps" class="r-stretch">
                    </section>

                    <section id="node_status">
                        <h1>Node Status</h1>
                        <span class="blue">Fully-expanded</span>: <span class="cyan">visited</span> (reward evaluated) all children nodes at least once<br>
                        <span class="skyblue">Not fully-expanded</span>: didn't visit all children nodes, further expansion is possible!<br>
                        Each node contains information about win rate, number of visits, ...
                        <img class="r-stretch" src="img/mcts_1.png" alt="MCTS">
                    </section>

                    <!-- detailed process with implementation:
                        Python: https://www.baeldung.com/java-monte-carlo-tree-search 
                        Java: https://ai-boson.github.io/mcts/
                    -->
                    <section id="mcts_select_1">
                        <h1>MCTS Process - Selection</h1>
                        <div class="row">
                            <div class="col-20 r-stretch"><img src="img/mcts_selection.png" alt="MCTS selection step"></div>
                            <div class="col-80">
                                <ul>
                                    <li><span class="strike"> 4 </span> 3 different type of nodes: <dd>
                                            -<span class="grey">unvisited</span>: never been evaluated before<br>
                                            -<span class="blue">fully-expanded</span>: as explained above (all children nodes have been visited)<br>
                                            -<span class="skyblue">not fully-expanded</span>: visited at least once, but not all children has been visited</dd>
                                    </li>
                                    <li class="fragment">As the game begins, all nodes are unvisited - <span class="orange">randomly pick one</span> child of the root to simulate.</li>
                                    <li class="fragment">Then, which node to choose?
                                        <dd>
                                    <li>Choose the one with <span class="red">biggest win rate</span> (Number of Win/Total Number)<span class="fragment purple">, is this a good choice?</span></li>
                                    </dd>
                                    </li>
                                </ul>
                            </div>
                        </div>
                    </section>

                    <section id="mcts_select_2">
                        <h1>MCTS Process - Selection (cont.)</h1>
                        <div class="row">
                            <div class="col-20 r-stretch"><img src="img/mcts_selection.png" alt="MCTS selection step"></div>
                            <div class="col-80">
                                <ul>
                                    <li>Which node to choose?
                                        <dd>
                                    <li>Choose the one with <span class="red">biggest win rate</span> (Number of Win/Total Number), <span class="purple">is this a good choice?</span>
                                    </li>
                                    <li class="fragment"><span class="red">No!</span> As the same node will be picked up all the time if it wins in the first random simulation.</span></li>
                                    </dd>
                                    </li>
                                    <li class="fragment">The selection strategy:
                                        <dd>- keep on selecting the best child node until reaching the leaf node<br>
                                            - <span class="smalltext lime">$UCT$</span> is a good measure for the selection<br>
                                    </li>
                                </ul>
                            </div>
                        </div>
                    </section>

                    <section id="mcts_expansion">
                        <h1>MCTS Process - Expansion</h1>
                        <div class="row">
                            <div class="col-20 r-stretch"><img src="img/mcts_expansion.png" alt="MCTS expansion step"></div>
                            <div class="col-80">
                                <ul>
                                    <li>When it can no longer apply <span class="lime">$UCT$</span> to find the successor node, <span class="orange">expand</span> the game tree by appending all possible children nodes from the leaf node.</li>
                                    <li>After Expansion, the algorithm picks <span class="red">ONE</span> child node arbitrarily.</li>
                                    <li>Empty statistics for the <span class="orange">picked node</span> - <span class="lime">expanded node</span></li>
                                </ul>
                            </div>
                        </div>
                    </section>

                    <section id="mcts_simulation">
                        <h1>MCTS Process - Simulation</h1>
                        <div class="row">
                            <div class="col-20 r-stretch"><img src="img/mcts_simulation.png" alt="MCTS simulation step"></div>
                            <div class="col-80">
                                <ul>
                                    <li>Simulates entire game from the <span class="orange">picked node</span> until it reaches the resulting state (playout process):
                                        <ul>
                                            <li>using Monte Carlo method to randomly simulate the game quickly (<span class="cyan">light playout</span>)</li>
                                            <li>applying complicated heuristics or evaluation functions (<span class="purple">heavy playout</span>)</li>
                                        </ul>
                                    </li>
                                    <li>Evaluate the game state to figure out which player os has won. For a given player it's<dd>- either <span class="lime">win (1)</span> or <span class="red">lost (0)</span></dd>
                                    </li>
                                </ul>
                            </div>
                        </div>
                    </section>

                    <section id="mcts_backpropagation">
                        <h1>MCTS Process - Backpropagation</h1>
                        <div class="row">
                            <div class="col-20 r-stretch r-stack">
                                <img class="fragment fade-in-then-out" data-fragment-index="1" src="img/mcts_simulation.png" alt="MCTS simulation step">
                                <img class="fragment fade-in" data-fragment-index="2" src="img/mcts_backpropagation.png" alt="MCTS simulation step">
                            </div>
                            <div class="col-80">
                                <ul>
                                    <li class="fragment" data-fragment-index="1">Assign the simulated results as the statistics of the <span class="orange">picked node</span></li>
                                    <li class="fragment" data-fragment-index="2">Update the information of the parent of the <span class="orange">picked node</span></li>
                                    <li class="fragment" data-fragment-index="2">Traverse upwards to the root and increments visit score for all visited nodes</li>
                                    <li class="fragment" data-fragment-index="2">The whole game tree has been updated</dd>
                                    </li>
                                </ul>
                            </div>
                        </div>
                    </section>

                    <section id="mcts_process_recall">
                        <h1>MCTS Process Recall</h1>
                        <img src="img/mctc_steps.png" alt="MCTS steps" class="r-stretch">
                        <p>MCTS keeps repeating these four phases until<br>
                            some fixed number of iterations <br>
                            or some fixed amount of time</p>
                    </section>

                    <section id="mcts_process_paper">
                        <h1>MCTS Process Algorithm</h1>
                        <img src="img/mcts_recall.png" alt="MCTS steps" class="r-stretch">
                        <p class="tinytext">source: <a href="https://www.researchgate.net/publication/235985858_A_Survey_of_Monte_Carlo_Tree_Search_Methods" target="_blank">https://www.researchgate.net/publication/235985858_A_Survey_of_Monte_Carlo_Tree_Search_Methods</a></p>
                    </section>

                    <section id="mcts_vis">
                        <h1>MCTS: Tic-Toc-Toe</h1>
                        <iframe src="https://vgarciasc.github.io/mcts-viz/" frameborder="0" class="float-left r-stretch" style="background-color: white;"></iframe>
                    </section>

                    <section id="mario_bro">
                        <h1>MCTS: Mario Brothers</h1>
                        <img class="r-stretch" src="img/mcts_demo_2.gif" alt="MCTS demo">
                        <p>MCTS can be used to deal with problems <br>in very large, complex game domains</p>
                    </section>
                </section>

                <section id="chap_end">
                    <section id="challenge">
                        <h1>Challenges for Tree Search <br>in Real-Time Domains</h1>
                        <ul>
                            <li>Many more <span class="blue">decisions per game</span> than most turn-based games</li>
                            <li>Hence state space is enormous even if branching factor is small
                                <ul>
                                    <li>19x19 Go game has of the order $10^{171}$ states</li>
                                    <li>Physical Travelling Salesman Problem has of the order $10^{1556}$ states</li>
                                </ul>
                            </li>
                            <li><span class="blue">Time budget</span> is restricted (milliseconds per decision)</li>
                        </ul>
                    </section>

                    <section id="conclusion">
                        <h1>Conclusion</h1>
                        <ul>
                            <li>MCTS is a powerful <span class="purple">general-purpose</span> AI technique<dd>- Asymmetric, Anytime, Aheuristic</dd>
                            </li>
                            <li>MCTS has proven successful in several challenging classes of games<dd>- games of imperfect information<br>
                                    - commercial mobile games<br>
                                    - real-time games</dd>
                            </li>
                            <li>It shows promise in many other games and non-game applications</li>
                        </ul>
                    </section>

                    <section id="further_reading">
                        <h1>Further Reading</h1>
                        <div class="row">
                            <div class="col-40"><a href="https://ieeexplore.ieee.org/document/6145622" target="_blank"><img class="img-zoom-2" src="img/mcts_paper.png" alt="MCTS paper"></a></div>
                            <div class="col-60 text-left">C. B. Browne et al., <br>
                                <a href="https://ieeexplore.ieee.org/document/6145622" target="_blank"><span class="blue">A Survey of Monte Carlo Tree Search Methods</span></a><br>
                                in IEEE Transactions on Computational Intelligence and AI in Games, <br>
                                vol. 4, no. 1, pp. 1-43, March 2012, <br>
                                doi: <a href="https://ieeexplore.ieee.org/document/6145622" target="_blank">10.1109/TCIAIG.2012.2186810</a>.
                            </div>
                        </div>
                    </section>

                    <section id="questions">
                        <h2><span class="bold purple">Questions?</span></h2>
                        <img src="../common/questions.PNG" class="r-stretch" alt="question image"><br>
                        <h5>Email: <a class="emaillink"></a></h5>
                        <span class="office cyan"></span>
                    </section>

                </section>
            </div>
        </div>

        <!-- header footer div -->
        <div id="header">
            <!-- <div id="header-left">HEADER-LEFT</div> -->
            <!-- <div id="header-right">HEADER-RIGHT</div> -->
            <div class="footer-left"><a href="http://www.falmouth.ac.uk" target="_blank"><img class="img-width-100 img-round-corner-10 img-zoom-15 fix-bottom-left" src="../common/logo.png" title="visit Falmouth University"></a></div>
            <!-- <div id="footer-right">FOOTER-RIGHT</div> -->
        </div>

        <!-- include all js files and settings -->
        <script id="all_js_files" src="../../../assets/dist/js/include.js"></script>

    </body>

</html>